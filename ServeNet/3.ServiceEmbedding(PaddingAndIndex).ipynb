{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Pading and Indexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import csv\n",
    "import h5py\n",
    "from pandas import HDFStore, read_hdf\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read from H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8123, 2)\n",
      "(2061, 2)\n"
     ]
    }
   ],
   "source": [
    "AllData = read_hdf('../Data/RandomSplittedByCatagories9.h5', key='AllData')\n",
    "TrainServices = read_hdf('../Data/RandomSplittedByCatagories9.h5', key='Train')\n",
    "TestServices = read_hdf('../Data/RandomSplittedByCatagories9.h5', key='Test')\n",
    "print(TrainServices.shape)\n",
    "print(TestServices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = AllData['Service Desciption']\n",
    "Y = AllData['Service Classification']\n",
    "\n",
    "Train_X = list(TrainServices['Service Desciption'])\n",
    "Train_Y = list(TrainServices['Service Classification'])\n",
    "\n",
    "Test_X = list(TestServices['Service Desciption'])\n",
    "Test_Y = list(TestServices['Service Classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Federal Reserve Bank of St. Louis is one of 12 regional Reserve banks in the Fed System. The Research Division of the bank looks to promote quality economic research and contribute to economic policy discussions. In support of this goal, it has created APIs that let developers access the data stored on its web site. The FRED API lets users query the Federal Reserve Economic Data (FRED) and Archival Federal Reserve Economic Data (ALFRED) databases to retrieve specific data. The requested data can be customized according to data source, release, category, series, and other preferences. The API uses RESTful calls and responses are formatted in XML.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text to index by Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word_to_index, index_to_word from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size in corpus: 23560\n",
      "vocab size in Glove: 400000\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab size in corpus: \" + str(vocab_size))\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../Data/glove.6B.200d.txt')\n",
    "\n",
    "print(\"vocab size in Glove:\", len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 8619\n"
     ]
    }
   ],
   "source": [
    "WordsNotInGlove = []\n",
    "\n",
    "for k, v in t.word_index.items():\n",
    "    if k not in word_to_index:\n",
    "        WordsNotInGlove.append(k)\n",
    "        \n",
    "print(\"length:\", len(WordsNotInGlove)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8619"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WordsNotInGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index[\"nasa's\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Glove5b to Index dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_word_to_index(X):\n",
    "\n",
    "    allservices = []\n",
    "\n",
    "    for s in X:\n",
    "\n",
    "        words = text_to_word_sequence(s)\n",
    "\n",
    "        service = []\n",
    "\n",
    "        for w in words:\n",
    "\n",
    "            if w in word_to_index:\n",
    "                index = word_to_index[w]\n",
    "                service.append(index)\n",
    "\n",
    "        allservices.append(service)\n",
    "\n",
    "    return allservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_Train_X = glove_word_to_index(Train_X)\n",
    "indexed_Test_X = glove_word_to_index(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3PlayMedia Advanced Timeshift API allows developers to caption timing changes in edited video sections. It supports advanced parameters for extracting in/out points from caption files. The API is applicable to any completed file in the 3PlayMedia system. 3play Media is a provider of captioning and transcription solutions for a variety of media platforms including online video, media & entertainment, education, e-learning, and market research.\n",
      "[357266, 46991, 359990, 57459, 52074, 122453, 360915, 91574, 360028, 96737, 188481, 133578, 378547, 323172, 193716, 348224, 46991, 277191, 151349, 143138, 188481, 272930, 287518, 154323, 91574, 148107, 357266, 57459, 192973, 57833, 360915, 57170, 106328, 148092, 188481, 357266, 350362, 239464, 192973, 43010, 294047, 268046, 91576, 54718, 363856, 337177, 151349, 43010, 376136, 268046, 239464, 286285, 188839, 270193, 378547, 239464, 137889, 133717, 132033, 219115, 54718, 234840, 306262]\n"
     ]
    }
   ],
   "source": [
    "print(Train_X[4])\n",
    "print(indexed_Train_X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Intrinio Algiers Stock Exchange Prices API offers end of day (EOD) prices from the Algiers Stock Exchange in Algeria. The API returns the most recent data point for a selected identifier such as stock market index symbol, and CIK ID. Additionally, the API offers professional-grade historical data, and lists of securities. Intrinio offers payment plans for individuals, startups, developers, and enterprises. Intrinio is a provider of financial services that delivers data collection, data entry, and data analysis tools.\n",
      "[357266, 51254, 343465, 142368, 292136, 57459, 268194, 136979, 268046, 117874, 138180, 292136, 154323, 357266, 51254, 343465, 142368, 188481, 51245, 357266, 57459, 307037, 357266, 250392, 302801, 117493, 287479, 151349, 43010, 323929, 186400, 346373, 60665, 343465, 234840, 189109, 349979, 54718, 101135, 186309, 46226, 357266, 57459, 268194, 293041, 165534, 179550, 117493, 54718, 223663, 268046, 323216, 268194, 279342, 286125, 151349, 189275, 341905, 122453, 54718, 137878, 192973, 43010, 294047, 268046, 148371, 325776, 357212, 120126, 117493, 104990, 117493, 138041, 54718, 117493, 54431, 361978]\n"
     ]
    }
   ],
   "source": [
    "print(Test_X[4])\n",
    "print(indexed_Test_X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Padding to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_padded_Train_X = pad_sequences(indexed_Train_X, maxlen=maxLen, padding='post')\n",
    "indexed_padded_Test_X = pad_sequences(indexed_Test_X, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Category to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(set(Y))\n",
    "categories_num = len(categories)\n",
    "index = list(range(len(categories)))\n",
    "index_to_category = dict(zip(index, categories)) \n",
    "category_to_index = dict(zip(categories, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10184, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Index = np.asarray([category_to_index[k] for k in Y])\n",
    "Y_one_hot = convert_to_one_hot(Y_Index, 50)\n",
    "Y_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_one_hot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8123, 50)\n",
      "(2061, 50)\n"
     ]
    }
   ],
   "source": [
    "Train_Y_Index = np.asarray([category_to_index[k] for k in Train_Y])\n",
    "Train_Y_one_hot = convert_to_one_hot(Train_Y_Index, 50)\n",
    "print(Train_Y_one_hot.shape)\n",
    "\n",
    "Test_Y_Index = np.asarray([category_to_index[k] for k in Test_Y])\n",
    "Test_Y_one_hot = convert_to_one_hot(Test_Y_Index, 50)\n",
    "print(Test_Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save category indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../Data/category_to_index.json', 'w') as fp:\n",
    "    json.dump(category_to_index, fp)\n",
    "    \n",
    "with open('../Data/index_to_category.json', 'w') as fp:\n",
    "    json.dump(index_to_category, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Datasets (np to h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('../Data/SplittedPaddedIndexedServiceDataset.h5', 'w')\n",
    "h5f.create_dataset('indexed_padded_Train_X', data=indexed_padded_Train_X)\n",
    "h5f.create_dataset('Train_Y_one_hot', data=Train_Y_one_hot)   \n",
    "h5f.create_dataset('indexed_padded_Test_X', data=indexed_padded_Test_X)\n",
    "h5f.create_dataset('Test_Y_one_hot', data=Test_Y_one_hot)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8123, 110) (8123, 50)\n",
      "(2061, 110) (2061, 50)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('../Data/SplittedPaddedIndexedServiceDataset.h5','r')\n",
    "indexed_padded_Train_X = h5f['indexed_padded_Train_X'][:]\n",
    "Train_Y_one_hot = h5f['Train_Y_one_hot'][:]\n",
    "indexed_padded_Test_X = h5f['indexed_padded_Test_X'][:]\n",
    "Test_Y_one_hot = h5f['Test_Y_one_hot'][:]\n",
    "print(indexed_padded_Train_X.shape, Train_Y_one_hot.shape)\n",
    "print(indexed_padded_Test_X.shape, Test_Y_one_hot.shape)\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
